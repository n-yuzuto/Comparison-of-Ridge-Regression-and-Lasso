# Comparison-of-Ridge-Regression-and-Lasso
Wineデータを訓練事例とテスト事例に分割し、訓練事例でridge回帰とLASSOを学習させました。  
正則化パラメータ(alpha)は2^-16,2^-15,...,2^12まで変化させ次の3つを出力しました。
 - 正則化パラメータ
 - 各特徴量とそれに対応する係数(昇順にソート)
 - 訓練誤差およびテスト誤差.  
 
 ## 考察
 テスト誤差が最小になる正則化パラメータにおいては、ラッソのテスト誤差がリッジ回帰よりわずかに小さいが、これはデータセットの選び方で変動するほどの非常に小さな差であり、ラッソとリッジ回帰の差はほとんどないと言える。  
 
 テスト誤差が最小になる正則化パラメータにおけるラッソでは、対応するモデルパラメータがゼロとなる特徴は存在せず、この設定では特徴選択の効果は見られていない。

## まとめ
【ラッソ回帰】   
〇一部のパラメータがゼロになる。→特徴選択できる。  
×微分できない（推定で求めるようです）。  

【リッジ回帰】  
〇なめらかなモデルが得られる。  
〇微分可能（解析的に解ける）。  
×パラメータはゼロにならないので、モデルが複雑になりやすい。  




***
.ipynbファイルが開かれない時は、こちらのリンクにURLを貼ってご覧になってください。  
[nbviewer](https://nbviewer.jupyter.org/)
